# BERT_Fine-tuning
üöÄ Introduction
This project trains a BERT-based model to classify consumer complaints into different product categories. The dataset consists of consumer complaints and their corresponding product labels.

The dataset is a CSV file (consumer_complaints.csv), containing:

Column	Description
consumer_complaint_narrative	Consumer's complaint text
product	Target category (label)

üìå Model Architecture
The model is a BERT-based text classifier:

Uses BERT-base-uncased for feature extraction
Extracts the [CLS] token for sentence representation
Adds a fully connected classification layer
Uses Dropout to prevent overfitting

üìå How to Run
1Ô∏è‚É£ Train the Model
Run the following command to start training:

Training Parameters:

Batch size: 16
Epochs: 3
Learning rate: 2e-5
Optimizer: Adam
Loss Function: CrossEntropyLoss

2Ô∏è‚É£ Evaluate the Model
To evaluate accuracy and AUC, run:

This computes:

Accuracy
AUC (One-vs-Rest Multi-class classification)

üìå Training Process
1Ô∏è‚É£ Load Dataset & Preprocess

Remove null values
Encode labels
Split into train (70%) / test (30%)
2Ô∏è‚É£ Tokenization

Use BERT Tokenizer (bert-base-uncased)
Convert text into tokenized format
Pad/truncate to max length (default: 128)
3Ô∏è‚É£ Create Dataset & DataLoader

Use ComplaintDataset for structured data
DataLoader loads batches efficiently
4Ô∏è‚É£ Train the BERT Model

Use train_epoch()
Apply Gradient Scaling (autocast) for efficiency
Update optimizer & scheduler
5Ô∏è‚É£ Evaluate the Model

Compute Validation Accuracy & Loss
Calculate AUC Score using roc_auc_score()

