BERT Consumer Complaint Classification

## ğŸš€ Introduction

This project trains a BERT-based model to classify consumer complaints into different product categories. The dataset consists of consumer complaints and their corresponding product labels.

## ğŸ“‚ Project Structure

â”œâ”€â”€ consumer_complaints.csv   # Dataset (Consumer complaints & product labels)
â”œâ”€â”€ model.py                  # BERT Classifier model definition
â”œâ”€â”€ dataset.py                # Custom Dataset class
â”œâ”€â”€ train.py                  # Training script
â”œâ”€â”€ evaluate.py               # Model evaluation script
â”œâ”€â”€ requirements.txt          # Required dependencies
â””â”€â”€ README.md                 # This documentation

## ğŸ”¨ Installation

1ï¸âƒ£ Set Up Virtual Environment

python -m venv env
source env/bin/activate  # On Linux/Mac
env\Scripts\activate     # On Windows

2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

## ğŸ“ Dataset

The dataset is a CSV file (consumer_complaints.csv), containing:


consumer_complaint_narrative


product (label)

## ğŸ¨ Model Architecture

The model is a BERT-based text classifier:

Uses BERT-base-uncased for feature extraction

Extracts the [CLS] token for sentence representation

Adds a fully connected classification layer

Uses Dropout to prevent overfitting

## ğŸ”„ How to Run

1ï¸âƒ£ Train the Model

Run the following command to start training:

python train.py

Training Parameters:

Batch size: 16

Epochs: 3

Learning rate: 2e-5

Optimizer: Adam

Loss Function: CrossEntropyLoss

2ï¸âƒ£ Evaluate the Model

To evaluate accuracy and AUC, run:

python evaluate.py

This computes:

Accuracy

AUC (One-vs-Rest Multi-class classification)

## ğŸŒŸ Training Process

âœ… Load Dataset & Preprocess

Remove null values

Encode labels

Split into train (70%) / test (30%)

âœ… Tokenization

Use BERT Tokenizer (bert-base-uncased)

Convert text into tokenized format

Pad/truncate to max length (default: 128)

âœ… Create Dataset & DataLoader

Use ComplaintDataset for structured data

DataLoader loads batches efficiently

âœ… Train the BERT Model

Use train_epoch()

Apply Gradient Scaling (autocast) for efficiency

Update optimizer & scheduler

âœ… Evaluate the Model

Compute Validation Accuracy & Loss

Calculate AUC Score using roc_auc_score()

## ğŸ“Š Model Evaluation

Metric

Description

Accuracy

Measures correct predictions

Loss

Cross-entropy loss for multi-class classification

AUC

Multi-class AUC (One-vs-Rest)

Example Output:

Epoch 1/3
Train loss 0.42  accuracy 87.5%
Val loss 0.40    accuracy 88.3%
AUC: 0.91

## ğŸš€ Future Improvements

âœ… Data Augmentation: Synonyms replacement, back-translationâœ… Hyperparameter Tuning: Learning rate, batch size optimizationâœ… More Advanced Models: Exploring BERT-large, RoBERTa, or DistilBERT

## ğŸ“ƒ References

BERT Paper: https://arxiv.org/abs/1810.04805

Hugging Face Transformers: https://huggingface.co/docs/transformers/index

Scikit-learn AUC: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html

## ğŸ‘¨â€ğŸ’» Author

## ğŸ‘¨â€ğŸ’» Ziqi Lin ğŸ“§ ziqi1229@umd.edu
